{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfe7ea52",
   "metadata": {},
   "source": [
    "## Classification pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7d6056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import mne\n",
    "from mne_bids import BIDSPath, read_raw_bids\n",
    "from typing import List\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "from scipy.signal import resample, butter, sosfiltfilt, iirnotch, tf2sos\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8771a97",
   "metadata": {},
   "source": [
    "## Preprocessing OTKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e02fc402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input(path: str,\n",
    "                  subject: str,\n",
    "                  task: str,\n",
    "                  channels: List[str],\n",
    "                  resampling_frq=200,\n",
    "                  notchfilter_frq=50,\n",
    "                  filter_bounds=[0.3, 75],\n",
    "                  verbose=False):\n",
    "\n",
    "    # open data\n",
    "    print(f'>>>>>>>>preprocessing {subject}-{task}')\n",
    "    bids_path = BIDSPath(subject=subject,\n",
    "                         session='01',\n",
    "                         task=task,\n",
    "                         root=path\n",
    "                         )\n",
    "    raw = read_raw_bids(bids_path, extra_params={'preload': True}, verbose=verbose)\n",
    "    raw.set_montage('standard_1020')\n",
    "\n",
    "    # pick eeg channels\n",
    "    print('picking eeg channels...')\n",
    "    # ['T3', 'T4', 'T5', 'T6'] channels that are only in the 10-20 system\n",
    "    # replaced with their equivalent name in the 10-10 system [T7, T8, P7, P8]\n",
    "    raw.pick(channels, verbose=verbose)\n",
    "\n",
    "    # interpolate bad channels if there is any\n",
    "    print('interpolating bad channels...')\n",
    "    raw.interpolate_bads(verbose=verbose)\n",
    "\n",
    "    # resampling\n",
    "    if resample is not None:\n",
    "        raw.resample(resampling_frq, verbose=verbose)\n",
    "\n",
    "    raw.filter(l_freq=filter_bounds[0], h_freq=filter_bounds[1], verbose=verbose)\n",
    "    raw.notch_filter((notchfilter_frq), verbose=verbose)\n",
    "    eeg_array = raw.get_data().T * 10**6  # volts to microvolts\n",
    "    points, chs = eeg_array.shape\n",
    "    a = points % (30 * 200)\n",
    "    eeg_array = eeg_array[60 * 200:-(a+60 * 200), :]\n",
    "    eeg_array = eeg_array.reshape(-1, 30, 200, chs)\n",
    "    eeg_array = eeg_array.transpose(0, 3, 1, 2)\n",
    "\n",
    "    return eeg_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "682090f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_path = '/Volumes/Extreme_SSD/PhD/OTKA_study1/EEG_data/BIDS/'\n",
    "behvioral_path = '../EEGModalNet/data/OTKA/PLB_HYP_data_MASTER.csv'\n",
    "\n",
    "behavioral = pd.read_csv(behvioral_path).dropna(subset='bids_id')\n",
    "behavioral['bids_id'] = behavioral['bids_id'].apply(lambda x: f'{int(x):02}')\n",
    "bid_id = behavioral['bids_id'].values\n",
    "behavioral.set_index('bids_id', inplace=True)\n",
    "gender = behavioral['gender']\n",
    "hypnotizability = behavioral['hypnotizability_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "869461fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = [f'{i:02}' for i in range(1, 52)]\n",
    "tasks = ['experience1', 'experience2', 'experience3', 'experience4']\n",
    "channels = ['Fp1', 'Fp2', 'F3', 'F4', 'C3', 'C4', 'P3', 'P4', 'O1', 'O2', 'F7', 'F8', 'T7', 'T8', 'P7', 'P8', 'Fz', 'Cz', 'Pz']\n",
    "\n",
    "def load_trial(path, subj, task):\n",
    "    eeg_data = process_input(\n",
    "        path=path,\n",
    "        subject=subj,\n",
    "        task=task,\n",
    "        channels=channels,\n",
    "        verbose=0\n",
    "    )\n",
    "    return eeg_data\n",
    "\n",
    "def process_subject_trials(path, subj, tasks):\n",
    "    trial_data = np.vstack(np.array([load_trial(path, subj, task) for task in tasks]))\n",
    "    clear_output()\n",
    "    return trial_data\n",
    "\n",
    "all_eeg_data = [process_subject_trials(eeg_path, subj, tasks) for subj in subjects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "262b909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two sessions are missing in the Last participants' data so we process and append it separately\n",
    "subj = '52'\n",
    "tasks = ['experience1', 'experience4']\n",
    "sub_52_data = process_subject_trials(eeg_path, subj, tasks)\n",
    "\n",
    "all_eeg_data.append(sub_52_data)\n",
    "\n",
    "# update subjects ids accordingly \n",
    "subjects = [f'{i:02}' for i in range(1, 53)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "98e5a687",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_to_be_excluded = {}\n",
    "\n",
    "for sub in range(len(all_eeg_data)):\n",
    "    sample_key = []\n",
    "    for i, sample, in enumerate(all_eeg_data[sub]):\n",
    "        if np.max(np.abs(sample)) > 100:\n",
    "            sample_key.append(i)\n",
    "    segments_to_be_excluded[f'sub_{sub}'] = sample_key\n",
    "\n",
    "# exclude bad epochs\n",
    "filtered_x = []\n",
    "\n",
    "for sub in range(len(all_eeg_data)):\n",
    "    excluded_segments = segments_to_be_excluded[f'sub_{sub}']\n",
    "    filtered_sub = np.delete(all_eeg_data[sub], excluded_segments, axis=0)  # Remove excluded segments\n",
    "    filtered_x.append(filtered_sub)\n",
    "\n",
    "# Convert the filtered list back to a numpy array\n",
    "x = np.array(filtered_x, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f2c83df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_segments = []\n",
    "subject_ids = []\n",
    "epoch_ids = []\n",
    "gender_of_epoch = []\n",
    "hypnotizablity_of_epoch = []\n",
    "\n",
    "for subj_idx, subj_array in zip(subjects, x):\n",
    "    for epoch_idx, segment in enumerate(subj_array):\n",
    "        all_segments.append(segment)\n",
    "        subject_ids.append(subj_idx)\n",
    "        gender_of_epoch.append(gender[subj_idx])\n",
    "        hypnotizablity_of_epoch.append(hypnotizability[subj_idx])\n",
    "        epoch_ids.append(epoch_idx)\n",
    "\n",
    "# Step 2: Convert list to array\n",
    "all_segments = np.stack(all_segments)  # shape (total_epochs, 30, 128)\n",
    "\n",
    "subject_epoch = [f'{sub_id}_epoch-{epo_id}' for sub_id, epo_id in zip(subject_ids, epoch_ids)]\n",
    "\n",
    "data = xr.DataArray(\n",
    "    all_segments,\n",
    "    dims=(\"subject_epoch\", \"channel\", \"segment\", \"time\"),\n",
    "    coords={\n",
    "        \"subject_epoch\": subject_epoch,\n",
    "        \"channel\": channels,\n",
    "        \"segment\": np.arange(all_segments.shape[2]),\n",
    "        \"time\": np.arange(all_segments.shape[3]),\n",
    "    },\n",
    "    attrs={'gender': gender_of_epoch,\n",
    "           'hypnotizablity': hypnotizablity_of_epoch},\n",
    "    name=\"eeg\"\n",
    ")\n",
    "\n",
    "# save\n",
    "# data.to_netcdf('data/OTKA_preprocessed_for_Cbramod.nc5', engine='h5netcdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd62ea3",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30e81363",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataarray('data/OTKA_preprocessed_for_Cbramod.nc5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339ecbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import torch \n",
    "from torch import nn\n",
    "from CBraMod.models.cbramod import CBraMod\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "DEFAULT_PARAMS = Namespace(**{\n",
    "    \"foundation_dir\": \"pretrained_weights/pretrained_weights.pth\",\n",
    "    \"features_file_path\": \"data/CBraMod_features_<DOWNSTREAM_TASK>.pt\",\n",
    "    \"num_of_classes\": 2,\n",
    "    \"device\": 'cpu',\n",
    "\n",
    "    \"data_dir\": \"data/LEMON/\",\n",
    "    \"channels\": ['Fp1', 'Fp2', 'F3', 'F4', 'C3', 'C4', 'P3', 'P4', 'O1', 'O2', 'F7', 'F8', 'T7', 'T8', 'P7', 'P8', 'Fz', 'Cz', 'Pz'],\n",
    "    \"downstream_task\": \"gender\",\n",
    "    \"segment_size\": 30,  # TODO\n",
    "    \"batch_size\": 1024,\n",
    "    \"bandpass_filter\": 0.3,\n",
    "    \"n_channels\": 19,\n",
    "    \"n_segments\": 2,  # TODO ?\n",
    "\n",
    "})\n",
    "\n",
    "DEFAULT_PARAMS.features_file_path = DEFAULT_PARAMS.features_file_path.replace(\n",
    "    \"<DOWNSTREAM_TASK>\", DEFAULT_PARAMS.downstream_task.lower())\n",
    "\n",
    "params = DEFAULT_PARAMS\n",
    "\n",
    "x = torch.tensor(ds.to_numpy()).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50740ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [01:45<00:00,  7.01s/it]\n"
     ]
    }
   ],
   "source": [
    "# feature extraction\n",
    "batch_size = 50  # Adjust batch size based on available memory\n",
    "num_batches = len(x) // batch_size + (1 if len(x) % batch_size != 0 else 0)\n",
    "\n",
    "backbone = CBraMod(\n",
    "    in_dim=200, out_dim=200, d_model=200,\n",
    "    dim_feedforward=800, seq_len=30,\n",
    "    n_layer=12, nhead=8\n",
    ")\n",
    "\n",
    "backbone.load_state_dict(\n",
    "    torch.load(params.foundation_dir,\n",
    "                map_location=torch.device(params.device), weights_only=True))\n",
    "\n",
    "backbone.eval()\n",
    "backbone.proj_out = nn.Identity()\n",
    "\n",
    "for batch_idx in tqdm(range(num_batches), leave=True):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = min((batch_idx + 1) * batch_size, len(x))\n",
    "    batch = x[start_idx:end_idx]\n",
    "    bz, ch_num, seq_len, patch_size = batch.shape\n",
    "    features = backbone(batch)\n",
    "    features = Rearrange('b c s p -> b (c s p)')(features).contiguous()\n",
    "    # store batch features\n",
    "    torch.save(features.detach(),\n",
    "               f\"data/OTKA_extracted_features/CBraMod_features_{batch_idx:02}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a287e3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = torch.zeros((x.shape[0], features.shape[1]))\n",
    "batch_size = 50\n",
    "for i, path_dir in enumerate(sorted(Path('data/OTKA_extracted_features/').glob('*.pt'))):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, len(x))\n",
    "    feature = torch.load(path_dir, weights_only=True)\n",
    "    all_features[start_idx:end_idx] = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e10dea2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open all the batch data, and concat them\n",
    "subject_ids = [i.split('_')[0] for i in ds.subject_epoch.values]\n",
    "gender = ds.gender\n",
    "gender = [0 if i=='Female' else 1 for i in gender]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a875264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features saved to data/LEMON/CBraMod_features_gender.pt\n"
     ]
    }
   ],
   "source": [
    "# store all features\n",
    "torch.save({'features': all_features,\n",
    "            'gender': gender,\n",
    "            'subject_ids': subject_ids}, params.features_file_path)\n",
    "print(\"features saved to\", params.features_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01660d5",
   "metadata": {},
   "source": [
    "## preprocessing Lemon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b9906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open lemon data\n",
    "\n",
    "# channels: ['T3', 'T4', 'T5', 'T6'] channels that are only in the 10-20 system replaced with their equivalent name in the 10-10 system [T7, T8, P7, P8].\n",
    "channels = ['Fp1', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8', 'T7', 'C3',\n",
    "            'Cz', 'C4', 'T8', 'P7', 'P3', 'Pz', 'P4', 'P8', 'O1', 'O2']  # FIXME: odering might be different in the data they inputed to their model\n",
    "\n",
    "n_subjects = 202\n",
    "lemon = xr.open_dataset('data/LEMON/eeg_eo_ec.nc5')\n",
    "\n",
    "x = lemon['eye_closed'].sel(subject=lemon.subject[:n_subjects], channel=channels).to_numpy() * 10**6  # Volts to microvolts\n",
    "\n",
    "# FIXME resampling to 200Hz\n",
    "# n_samples = int((x.shape[-1] / sampling_rate) * downsample_frq)\n",
    "# x = resample(x, num=n_samples, axis=-1)\n",
    "\n",
    "# bandpass filter\n",
    "sos = butter(4, 0.3, btype='highpass', fs=128, output='sos')\n",
    "x = sosfiltfilt(sos, x, axis=-1)\n",
    "\n",
    "# notchfilter\n",
    "fs = 128\n",
    "f0 = 50\n",
    "Q = 30\n",
    "w0 = f0 / (fs / 2)  # Normalized Frequency\n",
    "b, a = iirnotch(w0, Q)\n",
    "sos = tf2sos(b, a)\n",
    "x = sosfiltfilt(sos, x, axis=-1)\n",
    "\n",
    "subs, chs, points = x.shape\n",
    "\n",
    "# epoching\n",
    "x = x.reshape(subs, -1, chs, 30, 128)  # subjects, segments, channels, 30 seconds, sampling rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1602cf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_to_be_excluded = {}\n",
    "\n",
    "for sub in range(x.shape[0]):\n",
    "    sample_key = []\n",
    "    for i, sample, in enumerate(x[sub]):\n",
    "        if np.max(np.abs(sample)) > 100:\n",
    "            sample_key.append(i)\n",
    "    segments_to_be_excluded[f'sub_{sub}'] = sample_key\n",
    "\n",
    "# exclude bad epochs\n",
    "filtered_x = []\n",
    "\n",
    "for sub in range(x.shape[0]):\n",
    "    excluded_segments = segments_to_be_excluded[f'sub_{sub}']\n",
    "    filtered_sub = np.delete(x[sub], excluded_segments, axis=0)  # Remove excluded segments\n",
    "    filtered_x.append(filtered_sub)\n",
    "\n",
    "# Convert the filtered list back to a numpy array\n",
    "x = np.array(filtered_x, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6180f902",
   "metadata": {},
   "outputs": [],
   "source": [
    "demog = pd.read_csv('data/LEMON/Demographics.csv')\n",
    "gender = demog['Gender_ 1=female_2=male'].values\n",
    "age = demog['Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d767b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_segments = []\n",
    "subject_ids = []\n",
    "epoch_ids = []\n",
    "gender_of_epoch = []\n",
    "age_of_epoch = []\n",
    "\n",
    "for subj_idx, subj_array in enumerate(x):\n",
    "    for epoch_idx, segment in enumerate(subj_array):\n",
    "        all_segments.append(segment)\n",
    "        subject_ids.append(lemon.subject.values.tolist()[subj_idx])\n",
    "        gender_of_epoch.append(int(gender[subj_idx]))\n",
    "        age_of_epoch.append(age[subj_idx])\n",
    "        epoch_ids.append(epoch_idx)\n",
    "\n",
    "# Step 2: Convert list to array\n",
    "all_segments = np.stack(all_segments)  # shape (total_epochs, 30, 128)\n",
    "\n",
    "subject_epoch = [f'{sub_id}_epoch-{epo_id}' for sub_id, epo_id in zip(subject_ids, epoch_ids)]\n",
    "\n",
    "data = xr.DataArray(\n",
    "    all_segments,\n",
    "    dims=(\"subject_epoch\", \"channel\", \"segment\", \"time\"),\n",
    "    coords={\n",
    "        \"subject_epoch\": subject_epoch,\n",
    "        \"channel\": channels,\n",
    "        \"segment\": np.arange(all_segments.shape[2]),\n",
    "        \"time\": np.arange(all_segments.shape[3]),\n",
    "    },\n",
    "    attrs={'gender': gender_of_epoch,\n",
    "           'age': age_of_epoch},\n",
    "    name=\"eeg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802123cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_netcdf('data/LEMON/lemon_preprocessed_for_cbramod.nc5', engine='h5netcdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MNE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
