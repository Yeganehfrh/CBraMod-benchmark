{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfe7ea52",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e7d6056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'torch'\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "import pandas as pd\n",
    "from mne_bids import BIDSPath, read_raw_bids\n",
    "from typing import List\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "from scipy.signal import resample, butter, sosfiltfilt, iirnotch, tf2sos\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8771a97",
   "metadata": {},
   "source": [
    "## Preprocessing OTKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e02fc402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input(path: str,\n",
    "                  subject: str,\n",
    "                  task: str,\n",
    "                  channels: List[str],\n",
    "                  resampling_frq=200,\n",
    "                  notchfilter_frq=50,\n",
    "                  filter_bounds=[0.3, 75],\n",
    "                  verbose=False,\n",
    "                  microvolts=False,\n",
    "                  pretrain=False):\n",
    "\n",
    "    # open data\n",
    "    print(f'>>>>>>>>preprocessing {subject}-{task}')\n",
    "    bids_path = BIDSPath(subject=subject,\n",
    "                         session='01',\n",
    "                         task=task,\n",
    "                         root=path\n",
    "                         )\n",
    "    raw = read_raw_bids(bids_path, extra_params={'preload': True}, verbose=verbose)\n",
    "    raw.set_montage('standard_1020')\n",
    "\n",
    "    # pick eeg channels\n",
    "    print('picking eeg channels...')\n",
    "    # ['T3', 'T4', 'T5', 'T6'] channels that are only in the 10-20 system\n",
    "    # replaced with their equivalent name in the 10-10 system [T7, T8, P7, P8]\n",
    "    raw.pick(channels, verbose=verbose)\n",
    "\n",
    "    # interpolate bad channels if there is any\n",
    "    print('interpolating bad channels...')\n",
    "    raw.interpolate_bads(verbose=verbose)\n",
    "\n",
    "    # resampling\n",
    "    if resample is not None:\n",
    "        raw.resample(resampling_frq, verbose=verbose)\n",
    "\n",
    "    raw.filter(l_freq=filter_bounds[0], h_freq=filter_bounds[1], verbose=verbose)\n",
    "    raw.notch_filter((notchfilter_frq), verbose=verbose)\n",
    "    eeg_array = raw.get_data().T\n",
    "    points, chs = eeg_array.shape\n",
    "    if microvolts:\n",
    "        eeg_array = eeg_array * 10**6\n",
    "    \n",
    "    if pretrain:  # pretrain mode follows their pretraining's preprocessing steps\n",
    "        a = points % (30 * 200)\n",
    "        eeg_array = eeg_array[60 * 200:-(a+60 * 200), :]\n",
    "        eeg_array = eeg_array.reshape(-1, 30, 200, chs)\n",
    "    \n",
    "    else:\n",
    "        eeg_array = eeg_array[:200*416, :]  # trim time dim so that it is dividable by 200 (time is just a few timepoints longer than this)\n",
    "        eeg_array = eeg_array.reshape(-1, 2, 200, chs)\n",
    "        eeg_array = eeg_array.reshape(-1, 2, 200, chs)\n",
    "\n",
    "    eeg_array = eeg_array.transpose(0, 3, 1, 2)\n",
    "\n",
    "    return eeg_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "682090f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_path = '/Volumes/Extreme_SSD/PhD/OTKA_study1/EEG_data/BIDS/'\n",
    "behvioral_path = '../EEGModalNet/data/OTKA/PLB_HYP_data_MASTER.csv'\n",
    "\n",
    "behavioral = pd.read_csv(behvioral_path).dropna(subset='bids_id')\n",
    "behavioral['bids_id'] = behavioral['bids_id'].apply(lambda x: f'{int(x):02}')\n",
    "bid_id = behavioral['bids_id'].values\n",
    "behavioral.set_index('bids_id', inplace=True)\n",
    "gender = behavioral['gender']\n",
    "hypnotizability = behavioral['hypnotizability_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869461fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAIN = False\n",
    "subjects = [f'{i:02}' for i in range(1, 52)]\n",
    "tasks = ['experience1', 'experience2', 'experience3', 'experience4']\n",
    "if PRETRAIN:\n",
    "    channels = ['Fp1', 'Fp2', 'F3', 'F4', 'C3', 'C4', 'P3', 'P4', 'O1', 'O2', 'F7', 'F8', 'T7', 'T8', 'P7', 'P8', 'Fz', 'Cz', 'Pz']\n",
    "else:\n",
    "    channels = ['O1', 'O2', 'P1', 'P2', 'C1', 'C2', 'F1', 'F2']\n",
    "\n",
    "def load_trial(path, subj, task):\n",
    "    eeg_data = process_input(\n",
    "        path=path,\n",
    "        subject=subj,\n",
    "        task=task,\n",
    "        channels=channels,\n",
    "        verbose=0,\n",
    "        pretrain=PRETRAIN\n",
    "    )\n",
    "    return eeg_data\n",
    "\n",
    "def process_subject_trials(path, subj, tasks):\n",
    "    trial_data = np.vstack(np.array([load_trial(path, subj, task) for task in tasks]))\n",
    "    clear_output()\n",
    "    return trial_data\n",
    "\n",
    "all_eeg_data = [process_subject_trials(eeg_path, subj, tasks) for subj in subjects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262b909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PRETRAIN:  # since appending this data will create inhomogenous dimension we will remove it in the test mode\n",
    "    # Two sessions are missing in the Last participants' data so we process and append it separately\n",
    "    subj = '52'\n",
    "    tasks = ['experience1', 'experience4']\n",
    "    sub_52_data = process_subject_trials(eeg_path, subj, tasks)\n",
    "\n",
    "    all_eeg_data.append(sub_52_data)\n",
    "\n",
    "    # update subjects ids accordingly \n",
    "    subjects = [f'{i:02}' for i in range(1, 53)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e5a687",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PRETRAIN:\n",
    "    segments_to_be_excluded = {}\n",
    "\n",
    "    for sub in range(len(all_eeg_data)):\n",
    "        sample_key = []\n",
    "        for i, sample, in enumerate(all_eeg_data[sub]):\n",
    "            if np.max(np.abs(sample)) > 100:\n",
    "                sample_key.append(i)\n",
    "        segments_to_be_excluded[f'sub_{sub}'] = sample_key\n",
    "\n",
    "    # exclude bad epochs\n",
    "    filtered_x = []\n",
    "\n",
    "    for sub in range(len(all_eeg_data)):\n",
    "        excluded_segments = segments_to_be_excluded[f'sub_{sub}']\n",
    "        filtered_sub = np.delete(all_eeg_data[sub], excluded_segments, axis=0)  # Remove excluded segments\n",
    "        filtered_x.append(filtered_sub)\n",
    "\n",
    "    # Convert the filtered list back to a numpy array\n",
    "    x = np.array(filtered_x, dtype=object)\n",
    "\n",
    "    all_segments = []\n",
    "    subject_ids = []\n",
    "    epoch_ids = []\n",
    "    gender_of_epoch = []\n",
    "    hypnotizablity_of_epoch = []\n",
    "\n",
    "    for subj_idx, subj_array in zip(subjects, x):\n",
    "        for epoch_idx, segment in enumerate(subj_array):\n",
    "            all_segments.append(segment)\n",
    "            subject_ids.append(subj_idx)\n",
    "            gender_of_epoch.append(gender[subj_idx])\n",
    "            hypnotizablity_of_epoch.append(hypnotizability[subj_idx])\n",
    "            epoch_ids.append(epoch_idx)\n",
    "\n",
    "    # Step 2: Convert list to array\n",
    "    all_segments = np.stack(all_segments)  # shape (total_epochs, 30, 128)\n",
    "\n",
    "    subject_epoch = [f'{sub_id}_epoch-{epo_id}' for sub_id, epo_id in zip(subject_ids, epoch_ids)]\n",
    "\n",
    "    data = xr.DataArray(\n",
    "        all_segments,\n",
    "        dims=(\"subject_epoch\", \"channel\", \"segment\", \"time\"),\n",
    "        coords={\n",
    "            \"subject_epoch\": subject_epoch,\n",
    "            \"channel\": channels,\n",
    "            \"segment\": np.arange(all_segments.shape[2]),\n",
    "            \"time\": np.arange(all_segments.shape[3]),\n",
    "        },\n",
    "        attrs={'gender': gender_of_epoch,\n",
    "            'hypnotizablity': hypnotizablity_of_epoch},\n",
    "        name=\"eeg\"\n",
    "    )\n",
    "\n",
    "    # save\n",
    "    # data.to_netcdf('data/OTKA_preprocessed_for_Cbramod.nc5', engine='h5netcdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543556ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not PRETRAIN:\n",
    "    all_segments = np.array(all_eeg_data)\n",
    "\n",
    "    # removing last participant's data\n",
    "    hypnotizability_ = hypnotizability.iloc[:-1].values\n",
    "    gender_ = gender.iloc[:-1].values\n",
    "\n",
    "    data = xr.DataArray(\n",
    "        all_segments,\n",
    "        dims=(\"subject\", \"epoch\", \"channel\", \"segment\", \"time\"),\n",
    "        coords={\n",
    "            \"subject\": subjects,\n",
    "            \"epoch\": np.arange(all_segments.shape[1]),\n",
    "            \"channel\": channels,\n",
    "            \"segment\": np.arange(all_segments.shape[3]),\n",
    "            \"time\": np.arange(all_segments.shape[4]),\n",
    "        },\n",
    "        attrs={'gender': gender_,\n",
    "            'hypnotizablity': hypnotizability_},\n",
    "        name=\"eeg\"\n",
    "    )\n",
    "\n",
    "    # data.to_netcdf('data/OTKA_preprocessed_for_Cbramod_downstream.nc5', engine='h5netcdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd62ea3",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30e81363",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataarray('data/OTKA_preprocessed_for_Cbramod.nc5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339ecbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import torch \n",
    "from torch import nn\n",
    "from CBraMod.models.cbramod import CBraMod\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "DEFAULT_PARAMS = Namespace(**{\n",
    "    \"foundation_dir\": \"pretrained_weights/pretrained_weights.pth\",\n",
    "    \"features_file_path\": \"data/CBraMod_features_<DOWNSTREAM_TASK>.pt\",\n",
    "    \"num_of_classes\": 2,\n",
    "    \"device\": 'cpu',\n",
    "\n",
    "    \"data_dir\": \"data/LEMON/\",\n",
    "    \"channels\": ['Fp1', 'Fp2', 'F3', 'F4', 'C3', 'C4', 'P3', 'P4', 'O1', 'O2', 'F7', 'F8', 'T7', 'T8', 'P7', 'P8', 'Fz', 'Cz', 'Pz'],\n",
    "    \"downstream_task\": \"gender\",\n",
    "    \"segment_size\": 30,  # TODO\n",
    "    \"batch_size\": 1024,\n",
    "    \"bandpass_filter\": 0.3,\n",
    "    \"n_channels\": 19,\n",
    "    \"n_segments\": 2,  # TODO ?\n",
    "\n",
    "})\n",
    "\n",
    "DEFAULT_PARAMS.features_file_path = DEFAULT_PARAMS.features_file_path.replace(\n",
    "    \"<DOWNSTREAM_TASK>\", DEFAULT_PARAMS.downstream_task.lower())\n",
    "\n",
    "params = DEFAULT_PARAMS\n",
    "\n",
    "x = torch.tensor(ds.to_numpy()).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50740ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [01:45<00:00,  7.01s/it]\n"
     ]
    }
   ],
   "source": [
    "# feature extraction\n",
    "batch_size = 50  # Adjust batch size based on available memory\n",
    "num_batches = len(x) // batch_size + (1 if len(x) % batch_size != 0 else 0)\n",
    "\n",
    "backbone = CBraMod(\n",
    "    in_dim=200, out_dim=200, d_model=200,\n",
    "    dim_feedforward=800, seq_len=30,\n",
    "    n_layer=12, nhead=8\n",
    ")\n",
    "\n",
    "backbone.load_state_dict(\n",
    "    torch.load(params.foundation_dir,\n",
    "                map_location=torch.device(params.device), weights_only=True))\n",
    "\n",
    "backbone.eval()\n",
    "backbone.proj_out = nn.Identity()\n",
    "\n",
    "for batch_idx in tqdm(range(num_batches), leave=True):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = min((batch_idx + 1) * batch_size, len(x))\n",
    "    batch = x[start_idx:end_idx]\n",
    "    bz, ch_num, seq_len, patch_size = batch.shape\n",
    "    features = backbone(batch)\n",
    "    features = Rearrange('b c s p -> b (c s p)')(features).contiguous()\n",
    "    # store batch features\n",
    "    torch.save(features.detach(),\n",
    "               f\"data/OTKA_extracted_features/CBraMod_features_{batch_idx:02}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a287e3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = torch.zeros((x.shape[0], features.shape[1]))\n",
    "batch_size = 50\n",
    "for i, path_dir in enumerate(sorted(Path('data/OTKA_extracted_features/').glob('*.pt'))):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, len(x))\n",
    "    feature = torch.load(path_dir, weights_only=True)\n",
    "    all_features[start_idx:end_idx] = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e10dea2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open all the batch data, and concat them\n",
    "subject_ids = [i.split('_')[0] for i in ds.subject_epoch.values]\n",
    "gender = ds.gender\n",
    "gender = [0 if i=='Female' else 1 for i in gender]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a875264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features saved to data/LEMON/CBraMod_features_gender.pt\n"
     ]
    }
   ],
   "source": [
    "# store all features\n",
    "torch.save({'features': all_features,\n",
    "            'gender': gender,\n",
    "            'subject_ids': subject_ids}, params.features_file_path)\n",
    "print(\"features saved to\", params.features_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6734297a",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1e36179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pn/qy0qc5tx7t7_97xjmy6pvksc0000gn/T/ipykernel_18660/2302037879.py:4: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  X_e = np.array(data['features'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "DO_DOWN_SAMPLE = False\n",
    "data = torch.load('data/OTKA_extracted_features/CBraMod_features_gender.pt', weights_only=True)\n",
    "X_e = np.array(data['features'])\n",
    "y = np.array(data['gender'])\n",
    "groups = np.array(data['subject_ids'])\n",
    "\n",
    "if DO_DOWN_SAMPLE:\n",
    "    y_1_idx = np.where(y==1)[0]\n",
    "    # we know that there is less male (class 1) than female, so we down sample based on the lenght of data in class 1\n",
    "    y_0_idx = np.where(y==0)[0][:len(y_1_idx)]\n",
    "    all_idx = np.append(y_0_idx, y_1_idx)\n",
    "\n",
    "    # now we downsample features, subject ids and gender based on these indeces\n",
    "    X_e = X_e[all_idx]\n",
    "    y = y[all_idx]\n",
    "    groups = groups[all_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d0a167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.5),\n",
       " np.float64(0.5059288537549407),\n",
       " np.float64(0.48623853211009177))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SPLIT\n",
    "from sklearn.model_selection import ShuffleSplit, GroupShuffleSplit\n",
    "DO_GROUPED_SHUFFLE = True\n",
    "\n",
    "if DO_GROUPED_SHUFFLE:\n",
    "    group_shuffle = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "    train_idx, val_idx = next(group_shuffle.split(X_e, y, groups=groups))\n",
    "else:\n",
    "    random_shuffle = ShuffleSplit(n_splits=1, test_size=0.3, random_state=1)\n",
    "    train_idx, val_idx = next(random_shuffle.split(X_e, y))\n",
    "\n",
    "y.mean(), y[train_idx].mean(), y[val_idx].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c4c99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 713ms/step - accuracy: 0.4882 - loss: 3.2566 - val_accuracy: 0.5138 - val_loss: 7.8372\n",
      "Epoch 2/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 688ms/step - accuracy: 0.4960 - loss: 8.1228 - val_accuracy: 0.5138 - val_loss: 7.8372\n",
      "Epoch 3/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 627ms/step - accuracy: 0.5013 - loss: 8.0388 - val_accuracy: 0.5138 - val_loss: 7.8372\n",
      "Epoch 4/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 651ms/step - accuracy: 0.4908 - loss: 8.2067 - val_accuracy: 0.5138 - val_loss: 7.8372\n",
      "Epoch 5/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 654ms/step - accuracy: 0.4830 - loss: 8.3326 - val_accuracy: 0.5138 - val_loss: 7.8372\n",
      "Epoch 6/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 602ms/step - accuracy: 0.4960 - loss: 8.1228 - val_accuracy: 0.5138 - val_loss: 7.8372\n",
      "Epoch 7/500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 718ms/step - accuracy: 0.4908 - loss: 8.2067 - val_accuracy: 0.5138 - val_loss: 7.8372\n",
      "Epoch 8/500\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 445ms/step - accuracy: 0.5391 - loss: 7.4294"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 21\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# cls_model = keras.Sequential([\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#     # layers.Dense(256, activation='gelu'),\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#     # layers.Dense(128, activation='gelu'),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#     layers.Dense(1, activation='sigmoid')  # Binary classification\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ])\u001b[39;00m\n\u001b[1;32m     16\u001b[0m cls_model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m     17\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m),\n\u001b[1;32m     18\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     19\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 21\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mcls_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_e\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_e\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# class_weight=class_weights,\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/MNE/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/MNE/lib/python3.12/site-packages/keras/src/backend/torch/trainer.py:257\u001b[0m, in \u001b[0;36mTorchTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, data \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;66;03m# Callbacks\u001b[39;00m\n\u001b[1;32m    255\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 257\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Callbacks\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/MNE/lib/python3.12/site-packages/keras/src/backend/torch/trainer.py:117\u001b[0m, in \u001b[0;36mTorchTrainer.make_train_function.<locals>.one_step_on_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m data \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/MNE/lib/python3.12/site-packages/keras/src/backend/torch/trainer.py:72\u001b[0m, in \u001b[0;36mTorchTrainer.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 72\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model does not have any trainable weights.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/MNE/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:448\u001b[0m, in \u001b[0;36mBaseOptimizer.apply\u001b[0;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[1;32m    445\u001b[0m     grads \u001b[38;5;241m=\u001b[39m [g \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m g \u001b[38;5;241m/\u001b[39m scale \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grads]\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# Apply gradient updates.\u001b[39;00m\n\u001b[0;32m--> 448\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend_apply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;66;03m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;129;01min\u001b[39;00m trainable_variables:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/MNE/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:511\u001b[0m, in \u001b[0;36mBaseOptimizer._backend_apply_gradients\u001b[0;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_weight_decay(trainable_variables)\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;66;03m# Run update step.\u001b[39;00m\n\u001b[0;32m--> 511\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend_update_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_ema:\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_variables_moving_average(\n\u001b[1;32m    517\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainable_variables\n\u001b[1;32m    518\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/MNE/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/MNE/lib/python3.12/site-packages/keras/src/backend/torch/optimizers/torch_parallel_optimizer.py:10\u001b[0m, in \u001b[0;36mTorchParallelOptimizer._backend_update_step\u001b[0;34m(self, grads, trainable_variables, learning_rate)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;129m@torch_utils\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_backend_update_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads, trainable_variables, learning_rate):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parallel_update_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/MNE/lib/python3.12/site-packages/keras/src/backend/torch/optimizers/torch_adam.py:20\u001b[0m, in \u001b[0;36mAdam._parallel_update_step\u001b[0;34m(self, grads, variables, learning_rate)\u001b[0m\n\u001b[1;32m     18\u001b[0m dtype \u001b[38;5;241m=\u001b[39m variables[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m     19\u001b[0m lr \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mcast(learning_rate, dtype)\n\u001b[0;32m---> 20\u001b[0m local_step \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterations\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m, dtype)\n\u001b[1;32m     22\u001b[0m beta_1_power \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mpower(ops\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_1, dtype), local_step)\n\u001b[1;32m     23\u001b[0m beta_2_power \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mpower(ops\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_2, dtype), local_step)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/MNE/lib/python3.12/site-packages/keras/src/backend/common/variables.py:451\u001b[0m, in \u001b[0;36mVariable.__add__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__add__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/MNE/lib/python3.12/site-packages/keras/src/backend/torch/numpy.py:30\u001b[0m, in \u001b[0;36madd\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21madd\u001b[39m(x1, x2):\n\u001b[1;32m     29\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m convert_to_tensor(x1)\n\u001b[0;32m---> 30\u001b[0m     x2 \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39madd(x1, x2)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/MNE/lib/python3.12/site-packages/keras/src/backend/torch/core.py:207\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(x, dtype, sparse)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mas_tensor(x, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool, device\u001b[38;5;241m=\u001b[39mget_device())\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mfloat\u001b[39m):\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mas_tensor(\n\u001b[1;32m    210\u001b[0m         x, dtype\u001b[38;5;241m=\u001b[39mto_torch_dtype(floatx()), device\u001b[38;5;241m=\u001b[39mget_device()\n\u001b[1;32m    211\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras import regularizers, layers\n",
    "\n",
    "cls_model = keras.models.Sequential([   \n",
    "    layers.Dense(64, activation='gelu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "cls_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history = cls_model.fit(\n",
    "    X_e[train_idx], y[train_idx],\n",
    "    epochs=500,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_e[val_idx], y[val_idx]),\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01660d5",
   "metadata": {},
   "source": [
    "## preprocessing Lemon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b9906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open lemon data\n",
    "\n",
    "# channels: ['T3', 'T4', 'T5', 'T6'] channels that are only in the 10-20 system replaced with their equivalent name in the 10-10 system [T7, T8, P7, P8].\n",
    "channels = ['Fp1', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8', 'T7', 'C3',\n",
    "            'Cz', 'C4', 'T8', 'P7', 'P3', 'Pz', 'P4', 'P8', 'O1', 'O2']  # FIXME: odering might be different in the data they inputed to their model\n",
    "\n",
    "n_subjects = 202\n",
    "lemon = xr.open_dataset('data/LEMON/eeg_eo_ec.nc5')\n",
    "\n",
    "x = lemon['eye_closed'].sel(subject=lemon.subject[:n_subjects], channel=channels).to_numpy() * 10**6  # Volts to microvolts\n",
    "\n",
    "# FIXME resampling to 200Hz\n",
    "# n_samples = int((x.shape[-1] / sampling_rate) * downsample_frq)\n",
    "# x = resample(x, num=n_samples, axis=-1)\n",
    "\n",
    "# bandpass filter\n",
    "sos = butter(4, 0.3, btype='highpass', fs=128, output='sos')\n",
    "x = sosfiltfilt(sos, x, axis=-1)\n",
    "\n",
    "# notchfilter\n",
    "fs = 128\n",
    "f0 = 50\n",
    "Q = 30\n",
    "w0 = f0 / (fs / 2)  # Normalized Frequency\n",
    "b, a = iirnotch(w0, Q)\n",
    "sos = tf2sos(b, a)\n",
    "x = sosfiltfilt(sos, x, axis=-1)\n",
    "\n",
    "subs, chs, points = x.shape\n",
    "\n",
    "# epoching\n",
    "x = x.reshape(subs, -1, chs, 30, 128)  # subjects, segments, channels, 30 seconds, sampling rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1602cf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_to_be_excluded = {}\n",
    "\n",
    "for sub in range(x.shape[0]):\n",
    "    sample_key = []\n",
    "    for i, sample, in enumerate(x[sub]):\n",
    "        if np.max(np.abs(sample)) > 100:\n",
    "            sample_key.append(i)\n",
    "    segments_to_be_excluded[f'sub_{sub}'] = sample_key\n",
    "\n",
    "# exclude bad epochs\n",
    "filtered_x = []\n",
    "\n",
    "for sub in range(x.shape[0]):\n",
    "    excluded_segments = segments_to_be_excluded[f'sub_{sub}']\n",
    "    filtered_sub = np.delete(x[sub], excluded_segments, axis=0)  # Remove excluded segments\n",
    "    filtered_x.append(filtered_sub)\n",
    "\n",
    "# Convert the filtered list back to a numpy array\n",
    "x = np.array(filtered_x, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6180f902",
   "metadata": {},
   "outputs": [],
   "source": [
    "demog = pd.read_csv('data/LEMON/Demographics.csv')\n",
    "gender = demog['Gender_ 1=female_2=male'].values\n",
    "age = demog['Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d767b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_segments = []\n",
    "subject_ids = []\n",
    "epoch_ids = []\n",
    "gender_of_epoch = []\n",
    "age_of_epoch = []\n",
    "\n",
    "for subj_idx, subj_array in enumerate(x):\n",
    "    for epoch_idx, segment in enumerate(subj_array):\n",
    "        all_segments.append(segment)\n",
    "        subject_ids.append(lemon.subject.values.tolist()[subj_idx])\n",
    "        gender_of_epoch.append(int(gender[subj_idx]))\n",
    "        age_of_epoch.append(age[subj_idx])\n",
    "        epoch_ids.append(epoch_idx)\n",
    "\n",
    "# Step 2: Convert list to array\n",
    "all_segments = np.stack(all_segments)  # shape (total_epochs, 30, 128)\n",
    "\n",
    "subject_epoch = [f'{sub_id}_epoch-{epo_id}' for sub_id, epo_id in zip(subject_ids, epoch_ids)]\n",
    "\n",
    "data = xr.DataArray(\n",
    "    all_segments,\n",
    "    dims=(\"subject_epoch\", \"channel\", \"segment\", \"time\"),\n",
    "    coords={\n",
    "        \"subject_epoch\": subject_epoch,\n",
    "        \"channel\": channels,\n",
    "        \"segment\": np.arange(all_segments.shape[2]),\n",
    "        \"time\": np.arange(all_segments.shape[3]),\n",
    "    },\n",
    "    attrs={'gender': gender_of_epoch,\n",
    "           'age': age_of_epoch},\n",
    "    name=\"eeg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802123cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_netcdf('data/LEMON/lemon_preprocessed_for_cbramod.nc5', engine='h5netcdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MNE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
